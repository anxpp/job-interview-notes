# 容量管理设计

## 背景

### IT新基建提升资源使用率

IT新基建提升资源使用率的背景下，需要不断提升服务器、容器资源使用率，排除一些例外规则（如高可用等），整体资源使用率

### 现状

- 没有对整体容量资产分部、剩余容量占比、容量趋势及容量预警进行展示
- 容量规划无依据，新系统上线、IT资源扩容无预测分析手段
- 缺乏业务和系统视角的容量指标体系，无法准确反映业务和系统所分配的IT资源
- 缺乏当前容量资源与业务需求相匹配的分析手段；容量采集手段混乱无序，指标体系不完善，管理规范化不足。
- 资源分配易，回收难，造成容量实际利用率远低于当前值
- 没有可靠的成本分析途径

## 目标

- 减少资源浪费
- 高效的资源利用
- 支持服务水平监控和管理
- 工作负载管理
- 预测基础设计增长

### 成本控制

容量主动管理和优化，降低IT资源投入

确保最佳资源利用率、性能和成本

### 业务支撑

预测业务增长，容量自适应

## 容量管理分类

### 通用容量管理过程

![通用容量管理过程](容量管理通用流程.jpg)

容量管理是一个循环的过程，每个完整的过程执行结束，都需要根据调整后的容量信息重新定义阈值和基线

- 实施：可以结合现有规范，设置合适的初始参数
- 监控：监控是一个持续性的过程，可以结合现有的监控服务，采集需要的数据
- 分析：可以结合现有规范，通过一些算法分析各种优化建议
- 调优：对于调优报告，可以提交到现有的变更管理系统（如果有），通过标准流程进行

### 分类

容量管理主要分服务容量管理、资源容量管理、业务容量管理，过程描述如下：

![容量管理通用过程](容量管理过程.jpg)

## 服务容量管理

### 服务管理的过程

服务容量的管理存在于服务的整个生命周期

- 规划服务管理
- 服务容量的持续管理

### 规划服务管理

#### 标签

预设标签，为初始容量和选区提供参考

| 标签    | 说明                                   | 备注                                                                               |
|-------|--------------------------------------|----------------------------------------------------------------------------------|
| 计算型   | cpu密集型                               | 离线计算，跑批任务                                                                        |
| 内存型   | 需要大量内存                               | 缓存服务                                                                             |
| 业务高峰段 | 时间段：[14,15,16,17]                    | 如线上业务业务高峰期主要在白天，而离线计算高峰期主要在凌晨                                                    |
| 业务低峰段 | 时间段：[0,1,2,3,4,5]                    |                                                                                  |
| 服务规模  | 小型，中型，大型：small、middle、large          |                                                                                  |
| 服务环境  | test,uat,prod                        |                                                                                  |
| 峰值灵活性 | 1、2、3                                |                                                                                  |
| 容量级别  | 低、中、高                                |                                                                                  |
| 服务质量  | 低中高优：BestEffort、Burstable、Guaranteed | https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/ |

服务的以上标签，可在分析过程中不断更正和优化

*服务容量测试*

    现有规范：在新服务上线时需出具相关的测试报告

服务在上线前，测试不同并发等情况的资源消耗，进一步校准标签

针对不同类型的服务，规划合适的维度，用于监控

| 服务类型   | 参考维度          |
|--------|---------------|
| 数据库    | 存储、CPU、内存、连接数 |
| WEB服务  | CPU、内存，QPS，流量 |
| 缓存     |               |
| 消息队列   |               |
| ...... |               |

应该为不同类型的应用配置默认的阈值模板，再通过标签自动生成阈值的默认值，
因为不同标签的应用，即使类型相同，也应有不同的阈值配置（如Guaranteed级别的服务阈值应该更加保守）

当容量水位达到指定的指标需触发一定的操作

| 阈值     | 说明   | 备注   |
|--------|------|------|
| cpu    | 基线90 | 扩容   |
| cpu    | 基线80 | 扩容预警 |
| cpu    | 基线30 | 缩容预警 |
| cpu    | 基线20 | 缩容   |
| ...... |      |      |

### 服务容量的持续管理

#### 服务容量的实施过程

根据规划的相关标签和参数，自动生成初始容量用于初次部署

根据标签，评估应用的基线容量要求，实际可通过标签配置对应的模板，自动生成初始基线数据，输入如下

根据服务的类型和性质，估算各类资源的用量，并输出服务容量计划表

| 类别     | 数量      |     |
|--------|---------|-----|
| CPU    | 2000m   |     |
| 内存     | 2048Mi  |     |
| 存储     | 100Gi   |     |
| 副本数    | 2       |     |
| ...... |         |     |

#### 服务容量的监控过程

监控是一个持续的过程

针对不同类型的服务，监控各个维度的指标

收集和记录服务的各项指标水位

- 历史变化的水位线
- 水位线区间：高峰、低峰、平峰，按日、周、月、年、工作日、休息日
- 水位线高峰值、低峰值，按日、周、月、年、工作日、休息日
- 各水位线间的比值

*服务水位*

![服务水位](服务水位曲线图.png)

*峰值时段散点图*

#### 服务容量的分析过程

针对服务的阈值和基线，结合监控数据，通过适当的算法输出服务容量优化报告

*参数调整*

服务的初始参数，由于参考数据不足，可能存在参数设定不对或不准确

针对监控数据，对服务的标签和参数输出建设性的调整报告

*扩容*

对于容量的增长预测，如果短期内水位可能会突破阈值，输出扩容建议报告

*缩容*

如果水位长期位于地位，且峰值水位也较低，输出缩容建议报告

*下线*

如果监控中某些数据表示服务已经停用，则给出相关数据并输出下线报告

#### 服务容量的调优过程

针对优化报告，审核调优参数

反馈调优报告的有效性，优化调优报告的生成

根据当前状态，生成新的阈值和基线，审核通过后执行

调优依赖于基础功能的实现

*参数更新*

更新服务的标签、阈值等参数

*缩容*

缩容分横向缩容和纵向缩容

- 横向缩容：减少服务的副本数
- 纵向缩容：降低服务的配置

*扩容*

扩容同样分横向扩容和纵向扩容

- 横向扩容：增加服务的副本书
- 纵向扩容：增加服务配置

*服务下线*

服务下线可由不同场景触发：

- 主动下线：由业务方主动发起的下线操作，相关资源可以直接回收
- 被动下线：由分析过程而确定服务已不再使用，通知业务方确认并下线

被动下线的触发规则：可结合当前下线规则扩展（7天内流量为0）

| 指标  | 数值  | 说明  |
|-----|-----|-----|
|     |     |     |
|     |     |     |


![服务回收过程](服务回收过程.jpg)

*主动下线流程*

主动下线由业务方主动发起，确认后直接进行容量的回收即可

*被动下线流程*

被动下线需业务方严格校验

如果服务不应下线，则应对服务标签等做相应调整（如增加标签：0流量服务）

如果可以下线，则启动下线流程，完成后回收容量

## 资源容量管理

    包括：机房、机架、集群、主机等

### 规划资源容量管理

### 资源容量管理的过程

- 规划资源容量
- 实施容量：确定资源容量，指定合适的阈值和基线
- 监控：监控不容维度的使用率数据，应用的性能数据
    - 使用率数据
    - 性能数据
    - 峰值数据
- 分析：根据监控数据、设置的阈值和基线，分析出具容量优化的方案
    - 容量优化报告
- 调优：根据容量分析执行可行的优化方案，完成后根据新的当前状态实施新的阈值和基线并以此循环
    - 回收
    - 扩容

### 资源容量管理的规划和实施

对于不同级别的容量规模，会定义不同的阈值和基线

执行过程主要是根据当前容量信息定义阈值和基线，这是分析过程的重要参考

| 资源     | 容量    | 基线  | 扩容阈值 | 缩容阈值 |
|--------|-------|-----|------|------|
| CPU    | 488核  | 80% | 70%  | 20%  |
| 内存     | 1024G | 90% | 80%  | 30%  |
| 存储     |       |     |      |      |
| ...... |       |     |      |      |

### 资源容量管理的监控

监控过程主要监控容量的水位（使用率）变化，不同类别的容量有不同维度的评估维度

实时容量表格：

| 机房名称 | 机架数 | 主机数 | cpu容量 | 内存容量 | gpu容量 | cpu预备 | 内存预备容量 | gpu预备容量 | 趋势  |
|------|-----|-----|-------|------|-------|-------|--------|---------|-----|
|      |     |     |       |      |       |       |        |         |     |
|      |     |     |       |      |       |       |        |         |     |

| 机架名称 | 所属机房 | 主机数 | cpu容量 | 内存容量 | gpu容量 | cpu预备 | 内存预备容量 | gpu预备容量 |
|------|------|-----|-------|------|-------|-------|--------|---------|
|      |      |     |       |      |       |       |        |         |

| 物理集群名称 | 所属机房 | 主机数 | cpu容量 | 内存容量 | gpu容量 | cpu预备 | 内存预备容量 | gpu预备容量 |
|--------|------|-----|-------|------|-------|-------|--------|---------|
|        |      |     |       |      |       |       |        |         |

| 主机  | 所属集群 | cpu容量 | 内存容量 | gpu容量 | cpu预备 | 内存预备容量 | gpu预备容量 |
|-----|------|-------|------|-------|-------|--------|---------|
|     |      |       |      |       |       |        |         |

各维度的容量水位：

![水位](某维度资源池的容量水位.png)

- 峰值水位： 按天、周、月的峰值水位变化图，仅取对应时间段峰值
- 均值水位： 按天、周、月的峰值水位变化图，仅取对应时间段均值

使用服务的堆叠图展示容量的水位更佳

### 资源容量管理的分析

分析容量的水位情况，特别考虑峰值的情况，输出优化报告

报告中可能包含的内容：混合部署建议、扩容建议、缩容建议

#### 混合部署

结合资源容量水位的峰值数据和服务的峰值数据，调整服务的部署

![峰值堆叠](集群下服务水位峰值重叠.png)

![峰值堆叠](集群下服务水位峰值重叠0.png)

对于使用该资源的服务，服务的峰值应尽量不与资源水位的峰值重叠

典型的：

- 对于跑批（峰值多出现于夜间）和线上服务（峰值多出现于白天），部署到同一资源下是比较合理的方式

- 对于内存型和计算型的服务，也是比较好的混合部署方式

对于峰值与资源水位峰值重叠的服务，可尝试调整服务的实现，如不可调整，则反馈并打标属于峰值不可调整的服务

对于服务的部署（排除特殊需求的服务），根据自身的标签，系统应尽量推荐到合适的资源池中实施（如错峰部署）

对于峰值突出的资源池，可通过服务迁移的方式，与其他资源池平衡峰值

平衡情况：

![峰值平衡](集群下服务水位峰值平衡.png)

![峰值平衡](集群下服务水位峰值平衡0.png)

资源的水平峰值有所改善

优化过程：
- 对比同级资源池的水位历史情况
- 以峰值时段分类资源池
- 分好组后的资源池间，是可以相互进行平衡的，在组内资源池选择符合资源池峰值的服务
- 列出各资源池的建议迁移服务
- 输出报告：需包含以上步骤使用的详细数据

![峰值平衡](集群的水位.png)

如上，集群3的峰值与1、2错开，可从集群3现在合适的应用与集群1、2交换迁移

#### 扩容

预测资源水位的增长趋势，若短时间内可能会突破阈值，则输出建议扩建报告

扩容主要考虑峰值情况，不稳定总是发生在峰值水位的时候，这时候资源相对紧张

扩容优化报告的输出有两种方式触发：

- 预测容量水位(峰值)将要（比如7天）超过阈值(如80%)
- 容量水位连续(如连续一周峰值)超过阈值(如70%)

#### 缩容

对于水位长期低于缩容阈值，且峰值也不高的情况下，则输出建议缩容报告

#### 容灾级别分析

#### 基本参数调优

### 资源容量管理的调优

根据分析过程输出的报告，确定是否执行优化建议

完成后重新确认基线和阈值以及相关参数的调整

调优需要走变准的变更流程，涉及到具体业务的，需要业务方确认

#### 混合部署

混合部署主要是平衡不同资源池间的负载波动变化较大的情况，执行时涉及服务的迁移

针对混合部署的优化报告，初期由人工选择要平衡的资源池以及其中要迁移的服务执行，如果方案可行再逐渐替换为自动化执行

#### 扩容

扩容需要给出详细的报告，报告扩容的原因，然后进入变更流程（针对物理集群及以上规模时需要增加主机）

#### 回收

## 资源膨胀

资源膨胀主要使用逻辑集群实现，在各逻辑集群公用的主机上实现资源的超用

### 逻辑集群

## 业务容量管理

### 业务容量管理的过程

主要有规划容量管理，优化容量管理、控制容量管理

### 规划业务容量管理

业务容量使用租户实现

在业务线设立之初，应针对业务特性，合理的申请容量

（可以通过参数预设生成初始容量信息）

### 优化业务容量管理

### 控制业务容量管理

### 卡点

- 配合租户的服务资源申请和配额，现在租户的资源使用不超过配额限制

## 成本

通过对比租户的实际成本（实际资源使用）和当前成本（服务申请的资源），可以得出租户的资源申请是否合理

### 成本预算

### 成本分析

    服务、业务、资源

## 参考资料

[请问IT系统的容量管理应该如何规划 ?](https://www.zhihu.com/question/55363083)

[容量管理](https://zhuanlan.zhihu.com/p/463930536)

[IStorM CM容量管理平台](https://www.hatech.com.cn/h-pd-48.html)

[IStorM CM 容量管理](https://www.hatech.com.cn/h-col-174.html)

[云容量管理和应用](https://cloud.tencent.com/developer/article/1400119)

[云容量管理的重要性以及如何进行](https://www.techtarget.com/searchcloudcomputing/feature/The-importance-of-cloud-capacity-management-and-how-to-do-it#:~:text=Cloud%20capacity%20management%20is%20critical%20to%20an%20effective,that%20waste%20money%20and%20add%20unnecessary%20management%20overhead.)

[Intelligent Cloud Capacity Management]()

[智能云容量管理](https://www.researchgate.net/publication/236335710_Intelligent_Cloud_Capacity_Management)

[https://www.researchgate.net/publication/236335710_Intelligent_Cloud_Capacity_Management](https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/simplifying-private-cloud-capacity-management-paper-l.pdf)

[kubecost](https://github.com/kubecost/cost-model)

[如何在云原生混部场景下利用资源配额高效分配集群资源](https://zhuanlan.zhihu.com/p/504681375)
